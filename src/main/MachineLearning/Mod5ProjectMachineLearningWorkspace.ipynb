{"cells":[{"cell_type":"code","execution_count":14,"metadata":{"id":"7L6P5t2ErPBY","executionInfo":{"status":"ok","timestamp":1697795588401,"user_tz":-120,"elapsed":332,"user":{"displayName":"Dimitri von benckendorff","userId":"03752637868705301092"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"262d1259-fc84-440f-9bb3-126433267d15"},"outputs":[{"output_type":"stream","name":"stdout","text":["rm: cannot remove '*': No such file or directory\n"]}],"source":["# clean the current working directory\n","! cd /content\n","! rm -r *"]},{"cell_type":"markdown","metadata":{"id":"fVcGw0f88G1L"},"source":["## Introduction\n","In this notebook, we are developing a machine learning model that can classify some birds that are commonly spotted in the Netherlands. We have the following requirements for the model:\n","*   our model can classify images of commonly spotted birds\n","*   our model can classify images that do not show a bird as \"no bird\"\n","*   our model is developed using transfer learning.\n","*   our model achieves an accuracy of 95 percent or higher on classifying if it is a bird within the test set.\n","*   our model achieves an accuracy of ?? percent or higher on classifying the bird species within the test set.\n","\n","We follow the following structure to develop our model:\n","1. Select a bird image dataset\n","2. select a no bird image dataset\n","3. load in and split the data\n","5. select an open machine learning model for image classification\n","6. load the model and freeze hidden layers\n","5. apply transfer learning on said model using our dataset\n","6. test the accuracy of the trained model\n","7. if requirements are met, save the model to be used in our project"]},{"cell_type":"markdown","metadata":{"id":"42q0QdUpmUGe"},"source":["# 1. Bird image dataset\n","\n","We have selected an open dataset of bird images with their species as a\n","class. The bird dataset can be found [here](https://www.kaggle.com/datasets/davemahony/20-uk-garden-birds/). The dataset contains images of birds from 20 species that are commonly found in the UK. This is relevant to us because birds that are common in the UK overlap with the birds that are commonly spotted in The Netherlands."]},{"cell_type":"markdown","metadata":{"id":"K7MyrJYHnIcc"},"source":["# 2. No bird image dataset\n","\n","We have also selected an open dataset of tree images. The tree dataset can be found [here](https://www.kaggle.com/datasets/bryanb/forests-trees-and-leaves), the house dataset [here](https://www.kaggle.com/datasets/balraj98/facades-dataset) and the grass field dataset [here](https://www.kaggle.com/datasets/usharengaraju/grassclover-dataset). This is relevant to us because we want to train the machine learning model such that it can also classify and image as \"No bird\"."]},{"cell_type":"markdown","metadata":{"id":"BKT86vPPoAwu"},"source":["# 3. Load and split the data\n","\n","Now that we have relevant image data, we load this data into our workspace and split it into a train, validation, and test set. We add the \"No bird\" class to the data that was discussed in part 2.\n","\n","We want a good distribution of images per class after splitting the data. We want to split the images such that the ratio of images per class is the same in every set. So if we apply a 80-10-10 split, then we want 80 percent of the images per class in the training set, 10 percent of the images per class in the validation set, and 10 percent of the images per class in the test set."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":38},"id":"Gi5q7IYDsXBX","outputId":"5cf1e85d-d2d0-4eb5-d395-b53c4e4fb01f"},"outputs":[{"data":{"text/html":["\n","     <input type=\"file\" id=\"files-4705e2db-f1d5-4c79-b7bf-11cea0fce86d\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-4705e2db-f1d5-4c79-b7bf-11cea0fce86d\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["!pip install -q kaggle\n","\n","from google.colab import files\n","files.upload()\n","\n","! mkdir ~/.kaggle\n","! cp kaggle.json ~/.kaggle/\n","! chmod 600 ~/.kaggle/kaggle.json\n","\n","import kaggle"]},{"cell_type":"markdown","metadata":{"id":"cR4jPmQ1-Woi"},"source":["## Loading the bird dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21252,"status":"ok","timestamp":1697467191840,"user":{"displayName":"Dimitri von benckendorff","userId":"03752637868705301092"},"user_tz":-120},"id":"-XWTfmFtsXBZ","outputId":"b8eae6ef-3ab0-4c1d-8ba1-ed4b69c0c186"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading 20-uk-garden-birds.zip to /content\n"," 99% 374M/378M [00:11<00:00, 40.1MB/s]\n","100% 378M/378M [00:11<00:00, 35.0MB/s]\n","rm: cannot remove 'bird_data': No such file or directory\n","total 884\n","drwxr-xr-x  4 root root   4096 Oct 16 14:39 .\n","drwxr-xr-x  1 root root   4096 Oct 16 14:39 ..\n","-rw-r--r--  1 root root 865711 Jul 23 10:42 birds.csv\n","-rw-r--r--  1 root root  15095 Jul 23 10:42 stats.xlsx\n","drwxr-xr-x 22 root root   4096 Oct 16 14:39 withBackground\n","drwxr-xr-x 22 root root   4096 Oct 16 14:39 withoutBackground\n"]}],"source":["! kaggle datasets download -d davemahony/20-uk-garden-birds\n","\n","! rm -r bird_data\n","\n","! mkdir bird_data\n","\n","! unzip -q 20-uk-garden-birds.zip -d bird_data\n","\n","! ls bird_data -all"]},{"cell_type":"markdown","metadata":{"id":"MxyTNW8JH36Y"},"source":["## Loading the leaf dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9285,"status":"ok","timestamp":1697467201080,"user":{"displayName":"Dimitri von benckendorff","userId":"03752637868705301092"},"user_tz":-120},"id":"nTUqFNU1H36Z","outputId":"684f6fc7-5b97-421b-a496-9e161e91fd0f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading leaf-detection.zip to /content\n"," 98% 96.0M/98.3M [00:03<00:00, 41.1MB/s]\n","100% 98.3M/98.3M [00:03<00:00, 30.8MB/s]\n","rm: cannot remove 'leaf_data': No such file or directory\n","total 288\n","drwxr-xr-x 4 root root   4096 Oct 16 14:39 .\n","drwxr-xr-x 1 root root   4096 Oct 16 14:39 ..\n","drwxr-xr-x 3 root root   4096 Oct 16 14:39 test\n","drwxr-xr-x 2 root root  36864 Oct 16 14:40 train\n","-rw-r--r-- 1 root root 240282 Jun 22  2020 train.csv\n"]}],"source":["! kaggle datasets download -d alexo98/leaf-detection\n","\n","! rm -r leaf_data\n","\n","! mkdir leaf_data\n","\n","! unzip -q leaf-detection.zip -d leaf_data\n","\n","! ls leaf_data -all"]},{"cell_type":"markdown","metadata":{"id":"MTn6BFLi-XkR"},"source":["## Loading the house dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8566,"status":"ok","timestamp":1697467209638,"user":{"displayName":"Dimitri von benckendorff","userId":"03752637868705301092"},"user_tz":-120},"id":"fiEfYdto9_wi","outputId":"e7832af9-a65e-4a97-e15c-13fe62b16165"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading facades-dataset.zip to /content\n"," 92% 31.0M/33.5M [00:01<00:00, 28.1MB/s]\n","100% 33.5M/33.5M [00:01<00:00, 21.2MB/s]\n","rm: cannot remove 'house_data': No such file or directory\n","total 92\n","drwxr-xr-x 6 root root  4096 Oct 16 14:40 .\n","drwxr-xr-x 1 root root  4096 Oct 16 14:40 ..\n","-rw-r--r-- 1 root root 49004 Oct 17  2020 metadata.csv\n","drwxr-xr-x 2 root root  4096 Oct 16 14:40 testA\n","drwxr-xr-x 2 root root  4096 Oct 16 14:40 testB\n","drwxr-xr-x 2 root root 12288 Oct 16 14:40 trainA\n","drwxr-xr-x 2 root root 12288 Oct 16 14:40 trainB\n"]}],"source":["! kaggle datasets download -d balraj98/facades-dataset\n","\n","! rm -r house_data\n","\n","! mkdir house_data\n","\n","! unzip -q facades-dataset.zip -d house_data\n","\n","! ls house_data -all"]},{"cell_type":"markdown","metadata":{"id":"3VU9xkmdCxbm"},"source":["## Loading the grass dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":89790,"status":"ok","timestamp":1697467299395,"user":{"displayName":"Dimitri von benckendorff","userId":"03752637868705301092"},"user_tz":-120},"id":"YfyG4DcvC0Gv","outputId":"56ff586b-ccda-431b-b053-f82ba298e65a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading grassclover-dataset.zip to /content\n","100% 2.04G/2.04G [00:59<00:00, 33.3MB/s]\n","100% 2.04G/2.04G [00:59<00:00, 36.8MB/s]\n","rm: cannot remove 'grass_data': No such file or directory\n","total 16\n","drwxr-xr-x 3 root root 4096 Oct 16 14:41 .\n","drwxr-xr-x 1 root root 4096 Oct 16 14:41 ..\n","drwxr-xr-x 4 root root 4096 Oct 16 14:41 biomass_data\n"]}],"source":["! kaggle datasets download -d usharengaraju/grassclover-dataset\n","\n","! rm -r grass_data\n","\n","! mkdir grass_data\n","\n","! unzip -q grassclover-dataset.zip -d grass_data\n","\n","! ls grass_data -all"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39,"status":"ok","timestamp":1697467299395,"user":{"displayName":"Dimitri von benckendorff","userId":"03752637868705301092"},"user_tz":-120},"id":"34HM7Kr9xG3A","outputId":"20cb25e6-8c65-4467-a30c-7dea49fa512d"},"outputs":[{"name":"stdout","output_type":"stream","text":["rm: cannot remove 'data': No such file or directory\n"]}],"source":["! rm -r data\n","\n","! mkdir data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RnPDcaZv92db"},"outputs":[],"source":["import os\n","import shutil"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_AKRhBoE1cQ1"},"outputs":[],"source":["# Source directory\n","src_dir = \"./bird_data/withBackground/\"\n","\n","# Destination directory\n","dest_dir = \"./data/\"\n","\n","# Walk and move files from the source to destination directory with their new name\n","for root, dirs, files in os.walk(src_dir):\n","    for dir in dirs:\n","        try:\n","            shutil.rmtree(f\"./data/{dir.replace('_', '')}\", )\n","        except OSError:\n","            pass\n","        os.mkdir(f\"./data/{dir.replace('_', '')}\")\n","        for file in os.listdir(root + \"/\" + dir + \"/\"):\n","            src_file = os.path.join(root, dir, file)\n","            dest_file = os.path.join(dest_dir, dir.replace('_',''), file.replace('(','').replace(')',''))\n","            shutil.copy(src_file, dest_file)"]},{"cell_type":"markdown","metadata":{"id":"1qQYhvN4Clsc"},"source":["## Moving and renaming the tree, house, and grass data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O0py7jGtprSv"},"outputs":[],"source":["try:\n","    shutil.rmtree(\"./data/NoBird\")\n","except OSError:\n","    pass\n","os.mkdir(\"./data/NoBird\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fixz3_ZgIrx1"},"outputs":[],"source":["# Source directory\n","src_dir = \"./leaf_data/train/\"\n","\n","# Destination directory\n","dest_dir = \"./data/\"\n","\n","# Walk and move files from the source to destination directory with their new name\n","for (i,filename) in enumerate(os.listdir(src_dir),1):\n","    if i < 151:\n","        src_path = os.path.join(src_dir, filename)\n","        dest_path = os.path.join(dest_dir, \"NoBird\", f\"{i}.jpg\")\n","        shutil.copy(src_path, dest_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v0qoehYNCJ6G"},"outputs":[],"source":["# Source directory\n","src_dir = \"./house_data/trainA/\"\n","\n","# Destination directory\n","dest_dir = \"./data/\"\n","\n","# Walk and move files from the source to destination directory with their new name\n","for (i, filename) in enumerate(os.listdir(src_dir), 151):\n","    if i < 301:\n","        src_path = os.path.join(src_dir, filename)\n","        dest_path = os.path.join(dest_dir, \"NoBird\", f\"{i}.jpg\")\n","        shutil.copy(src_path, dest_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8yY14sOpFD7r"},"outputs":[],"source":["# Source directory\n","src_dir = \"./grass_data/biomass_data/test/images/\"\n","\n","# Destination directory\n","dest_dir = \"./data/\"\n","\n","# Walk and move files from the source to destination directory with their new name\n","for (i, filename) in enumerate(os.listdir(src_dir), 301):\n","    if i < 451:\n","        src_path = os.path.join(src_dir, filename)\n","        dest_path = os.path.join(dest_dir, \"NoBird\", f\"{i}.jpg\")\n","        shutil.copy(src_path, dest_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1697467307329,"user":{"displayName":"Dimitri von benckendorff","userId":"03752637868705301092"},"user_tz":-120},"id":"CdUW73WyFw8Q","outputId":"c5d65635-f3af-4143-974f-74daf2902538"},"outputs":[{"name":"stdout","output_type":"stream","text":["Directory 'Robin' contains 150 files.\n","Directory 'Wren' contains 150 files.\n","Directory 'Magpie' contains 92 files.\n","Directory 'Chaffinch' contains 150 files.\n","Directory 'Goldfinch' contains 150 files.\n","Directory 'HouseSparrow' contains 150 files.\n","Directory 'CoalTit' contains 144 files.\n","Directory 'LongTailedTit' contains 150 files.\n","Directory 'Blackbird' contains 150 files.\n","Directory 'CarrionCrow' contains 131 files.\n","Directory 'FeralPigeon' contains 93 files.\n","Directory 'GreatTit' contains 150 files.\n","Directory 'Bluetit' contains 150 files.\n","Directory 'Dunnock' contains 150 files.\n","Directory 'WoodPigeon' contains 150 files.\n","Directory 'NoBird' contains 450 files.\n","Directory 'Starling' contains 150 files.\n","Directory 'CollaredDove' contains 150 files.\n","Directory 'Jackdaw' contains 131 files.\n","Directory 'SongThrush' contains 150 files.\n","Directory 'Greenfinch' contains 150 files.\n","Total unique directories: 21\n"]}],"source":["import os\n","\n","# Directory path\n","directory = \"./data/\"\n","\n","# Initialize a dictionary to store directory counts\n","directory_counts = {}\n","\n","# Iterate through the subdirectories\n","for subdirectory in os.listdir(directory):\n","    subdirectory_path = os.path.join(directory, subdirectory)\n","\n","    if os.path.isdir(subdirectory_path):\n","        num_files = len(os.listdir(subdirectory_path))\n","        directory_counts[subdirectory] = num_files\n","\n","# Print the directory counts\n","for directory, count in directory_counts.items():\n","    print(f\"Directory '{directory}' contains {count} files.\")\n","\n","# Total count of unique directories\n","total_directories = len(directory_counts)\n","print(f\"Total unique directories: {total_directories}\")"]},{"cell_type":"markdown","metadata":{"id":"PL3UfmJeEXsL"},"source":["## Splitting the data into train, validation, and test sets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8DD8VmSHqWcw"},"outputs":[],"source":["try:\n","    shutil.rmtree(\"./split_data\")\n","except OSError:\n","    pass\n","os.mkdir(\"./split_data\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EKFBbhuakGlU"},"outputs":[],"source":["from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4246,"status":"ok","timestamp":1697467311548,"user":{"displayName":"Dimitri von benckendorff","userId":"03752637868705301092"},"user_tz":-120},"id":"KodDczd5kNsv","outputId":"84f3e1e8-f66e-4a10-aadc-89c670a2e64e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Data split completed.\n"]}],"source":["def split_data(ratio = (0.8, 0.1, 0.1)):\n","    src_dir = \"./data/\"\n","    dest_dir = \"./split_data/\"\n","\n","    classes = os.listdir(src_dir)\n","\n","    for class_name in classes:\n","        class_folder = os.path.join(src_dir, class_name)\n","        images = os.listdir(class_folder)\n","\n","        train_images, temp_images = train_test_split(images, test_size=(ratio[1] + ratio[2]))\n","        val_images, test_images = train_test_split(temp_images, test_size=ratio[2] / (ratio[1] + ratio[2]))\n","\n","        for dataset, dataset_name in zip([train_images, val_images, test_images], [\"train\", \"val\", \"test\"]):\n","            dataset_folder = os.path.join(dest_dir, dataset_name, class_name)\n","            os.makedirs(dataset_folder, exist_ok=True)\n","\n","            for image in dataset:\n","                source_path = os.path.join(class_folder, image)\n","                destination_path = os.path.join(dataset_folder, image)\n","                shutil.copy(source_path, destination_path)\n","\n","    print(\"Data split completed.\")\n","\n","split_data()"]},{"cell_type":"markdown","metadata":{"id":"Q71JSaIitewq"},"source":["## Augmenting our training and validation sets\n","\n","We want to develop a powerful image classification model using our few training and validation examples. That is why we have decided to augment them using Keras' ImageDataGenerator class. The images are augmented with a number of random transformations, so that our model would never see twice the exact same picture. This helps prevent overfitting and helps the model generalize better.\n","\n","Read more [here](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J9nrOIwPvJrD"},"outputs":[],"source":["try:\n","    shutil.rmtree(\"./generated_data\")\n","except OSError:\n","    pass\n","os.mkdir(\"./generated_data\")\n","os.mkdir(\"./generated_data/train\")\n","os.mkdir(\"./generated_data/val\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lksqDAlemUTZ"},"outputs":[],"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y01UIdqj4xzO"},"outputs":[],"source":["IMAGE_SHAPE = (1024, 1024)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44,"status":"ok","timestamp":1697467311549,"user":{"displayName":"Dimitri von benckendorff","userId":"03752637868705301092"},"user_tz":-120},"id":"ZIwl8A9D4xoA","outputId":"f03ad091-d9cb-42ff-d7d9-367c8cd32264"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 2630 images belonging to 21 classes.\n"]}],"source":["src_dir = \"./split_data/train\"\n","dest_dir = \"./generated_data/train\"\n","\n","train_datagen = ImageDataGenerator(\n","    rescale = 1./255,\n","    rotation_range=40,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")\n","\n","train_generator = train_datagen.flow_from_directory(\n","    src_dir,\n","    target_size=IMAGE_SHAPE,\n","    batch_size=5,\n","    class_mode='categorical',\n","    save_to_dir=dest_dir,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39,"status":"ok","timestamp":1697467311549,"user":{"displayName":"Dimitri von benckendorff","userId":"03752637868705301092"},"user_tz":-120},"id":"u74gHjzfkQwS","outputId":"86de2747-d3af-4864-cf89-1dfc4c728bf6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 328 images belonging to 21 classes.\n"]}],"source":["src_dir = \"./split_data/val\"\n","dest_dir = \"./generated_data/val\"\n","\n","val_datagen = ImageDataGenerator(rescale = 1./255)\n","\n","val_generator = val_datagen.flow_from_directory(\n","    src_dir,\n","    target_size=IMAGE_SHAPE,\n","    batch_size=5,\n","    class_mode='categorical',\n","    save_to_dir=dest_dir,\n",")"]},{"cell_type":"markdown","metadata":{"id":"sr7wP_SUcOJy"},"source":["#4. select and open machine learning model for image classification\n","\n","Why we use a BiT model for transfer learning:\n","\n","\"From architecture perspective BiT is nothing but a 4x times Scaled version of ResNet152V2. The main idea here is of Transfer Learning this model is pre-trained on a Large Dataset, so it can be trained on sub-datasets or basically other small datasets and as the model is pre-trained on a Very large Dataset it is expected that it will perform amazingly well on the small dataset.\"\n","\n","https://www.kaggle.com/datasets/utkarshsaxenadn/bitbirdspecies/data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3_JWzPXT_iKY"},"outputs":[],"source":["import numpy as np\n","import time\n","\n","import PIL.Image as Image\n","import matplotlib.pylab as plt\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","\n","import datetime\n","\n","%load_ext tensorboard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fTtQsRfa_35N","colab":{"base_uri":"https://localhost:8080/","height":245},"executionInfo":{"status":"error","timestamp":1697788597082,"user_tz":-120,"elapsed":278,"user":{"displayName":"Carmen Asbreuk","userId":"00981446011762154159"}},"outputId":"15d3e3cb-288e-4b74-e961-b8d53de54f60"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-37b814f906d4>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m classifier = tf.keras.Sequential([\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBiT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMAGE_SHAPE\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m ])\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'IMAGE_SHAPE' is not defined"]}],"source":["BiT =\"https://tfhub.dev/google/bit/m-r50x1/1\"\n","\n","classifier = tf.keras.Sequential([\n","    hub.KerasLayer(BiT, input_shape=IMAGE_SHAPE+(3,))\n","])\n","\n","classifier.add(tf.keras.layers.Dense(1001, activation='softmax'))\n","\n","classifier.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z0XcgEyMCPJ5","colab":{"base_uri":"https://localhost:8080/","height":193},"executionInfo":{"status":"error","timestamp":1697788594324,"user_tz":-120,"elapsed":279,"user":{"displayName":"Carmen Asbreuk","userId":"00981446011762154159"}},"outputId":"2f4b4dc1-c121-484b-d5dd-3ed463ddaa2e"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-3cdc5c5b409b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrace_hopper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrace_hopper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgrace_hopper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'grace_hopper' is not defined"]}],"source":["grace_hopper = np.array(grace_hopper)/255.0\n","grace_hopper.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FfNDikoLCZKR"},"outputs":[],"source":["result = classifier.predict(grace_hopper[np.newaxis, ...])\n","result.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GkIAscbNCpW1"},"outputs":[],"source":["predicted_class = tf.math.argmax(result[0], axis=-1)\n","predicted_class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"goI1rwBgC9sJ"},"outputs":[],"source":["labels_path = tf.keras.utils.get_file('ImageNetLabels.txt','https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')\n","imagenet_labels = np.array(open(labels_path).read().splitlines())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qb8cvR_IDBrk"},"outputs":[],"source":["plt.imshow(grace_hopper)\n","plt.axis('off')\n","predicted_class_name = imagenet_labels[predicted_class]\n","_ = plt.title(\"Prediction: \" + predicted_class_name.title())"]},{"cell_type":"markdown","metadata":{"id":"H2st1EZbax5Y"},"source":["We can save the model including weights and biases using <model>.save() or <model>.save_model() according to https://www.tensorflow.org/guide/keras/serialization_and_saving"]},{"cell_type":"markdown","metadata":{"id":"hCa3WTtEX9dV"},"source":["Creating and fitting the Mode (from https://www.kaggle.com/code/emreiekyurt/bird-species-classification-with-deep-learning)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4SgzbjBNX85p"},"outputs":[],"source":["# 1. Create a base model with tf.keras.applications\n","base_model = tf.keras.applications.InceptionV3(include_top= False,)\n","\n","# 2. Freeze the base model\n","base_model.trainable = False\n","\n","#3. Create inputs into models\n","inputs = tf.keras.layers.Input(shape =(300,300,3), name = \"input-layer\")\n","\n","#4. Rescaling\n","#x = tf.keras.layers.experimental.preprocessing.Rescaling(1/255.)(inputs)\n","\n","#5. Pass the inputs\n","x = base_model(inputs)\n","print(f\"Shape after passing inputs through base model: {x.shape}\")\n","\n","# 6. Average pool the outputs of the base model\n","x = tf.keras.layers.GlobalAveragePooling2D(name = \"global_average_pooling_layer\")(x)\n","print(f\"Shape after GlobalAveragePooling2D: {x.shape}\")\n","\n","#7. Create the output activation layer\n","outputs = tf.keras.layers.Dense(450, activation = \"softmax\", name = \"output-layer\")(x)\n","\n","# 8. Combine the inputs with outputs into a model\n","model_0 = tf.keras.Model(inputs, outputs)\n","\n","# 9. Compile the model\n","model_0.compile(loss = \"categorical_crossentropy\",\n","                optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01),\n","                metrics = [\"accuracy\"])\n","\n","\n","history = model_0.fit(train_data,\n","                                 epochs=10,\n","                                 steps_per_epoch = len(train_data),\n","                                 validation_data = val_data,\n","                                 validation_steps = int(0.25*len(val_data)),)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"00pxCm5VYOj3"},"outputs":[],"source":["model_0.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sUKQk8jgYPXC"},"outputs":[],"source":["model_0.evaluate(test_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P6kzewUdYPoZ"},"outputs":[],"source":["def plot_loss_curves(history):\n","\n","  loss = history.history[\"loss\"]\n","  val_loss = history.history[\"val_loss\"]\n","\n","  accuracy = history.history[\"accuracy\"]\n","  val_accuracy = history.history[\"val_accuracy\"]\n","\n","  epochs = range(len(history.history[\"loss\"]))\n","\n","  #plot loss\n","  plt.plot(epochs, loss, label = \"training_loss\")\n","  plt.plot(epochs, val_loss, label = \"val_loss\")\n","  plt.title(\"loss\")\n","  plt.xlabel(\"epochs\")\n","  plt.legend()\n","\n","  #plot accuracy\n","  plt.figure()\n","  plt.plot(epochs, accuracy, label = \"training_accuracy\")\n","  plt.plot(epochs, val_accuracy, label = \"val_accuracy\")\n","  plt.title(\"accuracy\")\n","  plt.xlabel(\"epochs\")\n","  plt.legend()\n","\n","plot_loss_curves(history)"]},{"cell_type":"markdown","metadata":{"id":"mVUafGr4YgEH"},"source":["Freeze top layers of Base Model (from the aforementioned notebook)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fw1jLBARYf0o"},"outputs":[],"source":["# To begin fine-tuning lets start by setting the last 10 layers as trainable\n","base_model.trainable = True\n","\n","# Un-freeze last 10 layers\n","for layer in base_model.layers[:-10]:\n","  layer.trainable = False\n","\n","# Recompile (we have to compile model every time there is a change)\n","model_0.compile(loss = \"categorical_crossentropy\",\n","                optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001), # when fine-tuning you typically want to lower lr by 10x\n","                 metrics = [\"accuracy\"] )\n","\n","# Check which layers are trainable\n","for layer_number, layer in enumerate(model_0.layers[1].layers):\n","  print(layer_number, layer.name, layer.trainable)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K9VhdP1hYolp"},"outputs":[],"source":["# Now we have unfrozen some of the layers on the top\n","print(len(model_0.trainable_variables))"]},{"cell_type":"markdown","metadata":{"id":"EBHJrswBZGTN"},"source":["Fine-tuning and Refitting (from the aforementioned notebook)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LqkaDl-oZJYO"},"outputs":[],"source":["initial_epochs = 10\n","fine_tune_epochs = initial_epochs + 1\n","\n","# Refit the model\n","history_2 = model_0.fit(train_data,\n","                       epochs = fine_tune_epochs,\n","                       validation_data = val_data,\n","                       validation_steps = int(0.25*len(val_data)),\n","                       initial_epoch =  history.epoch[-1],) # Start the epoch where it left before"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wiVqkbHqZknH"},"outputs":[],"source":["model_0.evaluate(test_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p1PQ6Jb_ZmJ6"},"outputs":[],"source":["plot_loss_curves(history_2)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}